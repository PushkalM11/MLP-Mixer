{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './../Models')\n",
    "from mlp_mixer import MLPMixer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_keys(state_dict, map_classifier, num_classes = 10):\n",
    "    keys = state_dict.keys()\n",
    "    new_keys = []\n",
    "    new_state_dict = {}\n",
    "\n",
    "    for key in keys:\n",
    "        new_key = key.replace(\"/\", \".\")\n",
    "        new_key = new_key.replace(\"MixerBlock_\", \"MixerBlock.\")\n",
    "        new_key = new_key.replace(\"channel_mixing.Dense_0\", \"channel_mixing.1.net.0\")\n",
    "        new_key = new_key.replace(\"channel_mixing.Dense_1\", \"channel_mixing.1.net.3\")\n",
    "        new_key = new_key.replace(\"token_mixing.Dense_0\", \"token_mixing.2.net.0\")\n",
    "        new_key = new_key.replace(\"token_mixing.Dense_1\", \"token_mixing.2.net.3\")\n",
    "        new_key = new_key.replace(\"LayerNorm_0\", \"token_mixing.0\")\n",
    "        new_key = new_key.replace(\"LayerNorm_1\", \"channel_mixing.0\")\n",
    "        new_key = new_key.replace(\"scale\", \"weight\")\n",
    "        new_key = new_key.replace(\"kernel\", \"weight\")\n",
    "        new_key = new_key.replace(\"stem\", \"stem.0\")\n",
    "        new_key = new_key.replace(\"head\", \"head.0\")\n",
    "        new_key = new_key.replace(\"pre_head.0_layer_norm\", \"pre_head_layer_norm\")\n",
    "        new_keys.append(new_key)\n",
    "    \n",
    "    if map_classifier:\n",
    "        for (key, new_key) in zip(keys, new_keys): \n",
    "            new_state_dict[new_key] = torch.tensor(state_dict[key], dtype = torch.float32).T\n",
    "    else:\n",
    "        for (key, new_key) in zip(keys, new_keys):\n",
    "            weights = torch.tensor(state_dict[key], dtype = torch.float32).T\n",
    "            if \"head.\" in new_key:\n",
    "                k = np.random.randint(0, weights.shape[0], num_classes)\n",
    "                # For bias\n",
    "                if len(weights.shape) == 1:\n",
    "                    weights = weights[k]\n",
    "                # For weights\n",
    "                else:\n",
    "                    weights = weights[k, :]\n",
    "            new_state_dict[new_key] = weights\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B/16 architecture\n",
    "net = MLPMixer(in_channels = 3,\n",
    "               dim = 768,\n",
    "               num_classes = 10,\n",
    "               patch_size = 16,\n",
    "               image_size = 224,\n",
    "               depth = 12,\n",
    "               token_dim = 384,\n",
    "               channel_dim = 3072).to(device)\n",
    "\n",
    "google_weights = np.load(\"./../Weights/imagenet1k-Mixer-B_16.npz\", allow_pickle = True)\n",
    "new_weights = convert_keys(google_weights, map_classifier = False, num_classes = 10)\n",
    "net.load_state_dict(new_weights, strict = False)\n",
    "\n",
    "non_linearity = nn.Softmax(dim = 1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.1)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./../Data\"\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "\n",
    "transform = T.Compose([\n",
    "            T.Resize(224),\n",
    "            T.ToTensor()])\n",
    "\n",
    "train_dataset = CIFAR10(root = data_dir, train = True, transform = transform, download = True)\n",
    "test_dataset = CIFAR10(root = data_dir, train = False, transform = transform, download = True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = shuffle, \n",
    "                          collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = shuffle,\n",
    "                          collate_fn = lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_train_loader = tqdm(train_loader, desc = f\"Train Epoch {epoch + 1}\")\n",
    "    net.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for (x, y) in tqdm_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = non_linearity(net(x))\n",
    "        loss_ = loss(y_hat, y)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        t_loss = loss_.item()\n",
    "        t_acc = (y_hat.argmax(1) == y).sum().item()\n",
    "        train_loss += t_loss / len(train_loader)\n",
    "        train_acc += t_acc / len(train_dataset)\n",
    "        tqdm_train_loader.set_postfix(train_loss = t_loss, train_acc = t_acc)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    tqdm_test_loader = tqdm(test_loader, desc = f\"Test Epoch {epoch + 1}\")\n",
    "    net.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for (x, y) in tqdm_test_loader:\n",
    "        y_hat = non_linearity(net(x))\n",
    "        loss_ = loss(y_hat, y)\n",
    "        t_loss = loss_.item()\n",
    "        t_acc = (y_hat.argmax(1) == y).sum().item()\n",
    "        test_loss += t_loss / len(test_loader)\n",
    "        test_acc += t_acc / len(test_dataset)\n",
    "        tqdm_test_loader.set_postfix(test_loss = t_loss, test_acc = t_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    print(f\"Epoch {epoch + 1} | Train Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | Test Loss {test_loss:.4f} | Test Acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(epochs) + 1, train_losses, label = \"Train Loss\")\n",
    "plt.plot(np.arange(epochs) + 1, test_losses, label = \"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(epochs) + 1, train_accs, label = \"Train Accuracy\")\n",
    "plt.plot(np.arange(epochs) + 1, test_accs, label = \"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLP_mixer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
